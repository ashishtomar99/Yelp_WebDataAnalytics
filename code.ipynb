{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_restaurant_url = \"https://www.yelp.com/biz/j-wata-temaki-bar-san-diego-2?osq=restaurant\" # define the restaurant url\n",
    "import urllib.request\n",
    "\n",
    "import time\n",
    "from urllib.request import FancyURLopener  # This is library that helps us create the headless browser\n",
    "from random import choice #This library helps pick a random item from a list\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
    "]\n",
    "\n",
    "# These are the usr agents for each of different browsers. Here we are using five, but it can be any number of user agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: MyOpener style of invoking requests is deprecated. Use newer urlopen functions/methods\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "class MyOpener(FancyURLopener, object):\n",
    "    version = choice(user_agents)\n",
    "# load the web page\n",
    "myopener = MyOpener()\n",
    "page=myopener.open(yelp_restaurant_url)\n",
    "\n",
    "# create empty list for stroring scraped data\n",
    "name = []\n",
    "review = []\n",
    "rest_name = []\n",
    "\n",
    "html2 = page.read().decode('utf-8') # generate html code for the web page\n",
    "\n",
    "last_page = False # create a flag\n",
    "\n",
    "while last_page == False: #until the last page is reached, do the following\n",
    "    \n",
    "    html4 = html2 #create copy of html code\n",
    "    html3 = html2\n",
    "    # find the restaurant name using specified dilimiter\n",
    "    start = html2.find('373c0__1F-Z6\\\">')\n",
    "    remaining = html2[start:]\n",
    "    end = remaining.find('</h1><div class')\n",
    "    rest_name.append(remaining[14:end])\n",
    "\n",
    "    # find reviewer names\n",
    "    while html3.find('\\\"author\\\" content=') != -1:\n",
    "        start1 = html3.find('\\\"author\\\" content=')\n",
    "        remaining = html3[start1:]\n",
    "        end1 = remaining.find('\\\">')\n",
    "        name.append(remaining[18: end1])\n",
    "        html3 = remaining[end1:]\n",
    "    # find review comments\n",
    "    while html2.find('<p itemprop=\\\"description\\\">') != -1:\n",
    "        start1 = html2.find('\\\"description\\\": ')\n",
    "        remaining = html2[start1:]\n",
    "        end1 = remaining.find('\\\", \\\"author\\\":')\n",
    "        if (remaining[15: end1]) != '':\n",
    "            review.append(remaining[16: end1])\n",
    "        html2 = remaining[end1:]\n",
    "\n",
    "    # check if the page consits of NEXT, which is a link to next review page\n",
    "    index = html4.find('<link rel=\\\"next\\\" href=\\\"')\n",
    "    if index == -1:\n",
    "        last_page = True\n",
    "    else:\n",
    "        remaining = html4[index:]\n",
    "        end = remaining.find('\\\" />')\n",
    "        url = remaining[23:end]\n",
    "        # update the html code with that of next page\n",
    "        html2 = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "\n",
    "# write the scraped data to csv file\n",
    "import csv\n",
    "with open('Files_Directory/Edgar/test.csv', mode='w') as output_file:\n",
    "    output_writer = csv.writer(output_file, delimiter=',', lineterminator='\\n')\n",
    "    output_writer.writerow(['restaurant_name', 'reviewer_name', 'review'])\n",
    "    for x in range(0, len(name)):\n",
    "        myData = [rest_name[0], name[x], review[x]]\n",
    "        output_writer.writerow(myData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\iamak\\\\Downloads\\\\Web Data codes'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
